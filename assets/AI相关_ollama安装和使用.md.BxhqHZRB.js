import{_ as l,c as e,o as t,ag as o}from"./chunks/framework.DWdSXOaE.js";const c=JSON.parse('{"title":"Ollama安装和使用","description":"","frontmatter":{},"headers":[],"relativePath":"AI相关/ollama安装和使用.md","filePath":"AI相关/ollama安装和使用.md","lastUpdated":1726751071000}'),p={name:"AI相关/ollama安装和使用.md"};function i(s,a,n,r,h,m){return t(),e("div",null,[...a[0]||(a[0]=[o('<h1 id="ollama安装和使用" tabindex="-1">Ollama安装和使用 <a class="header-anchor" href="#ollama安装和使用" aria-label="Permalink to &quot;Ollama安装和使用&quot;">​</a></h1><p>Ollama 是一种用于在本地部署和运行大语言模型（LLM）的工具或平台。与OpenAI提供的云端API不同，Ollama允许用户在本地计算资源上运行模型，从而提供更高的隐私性和可控性。以下是关于Ollama的一些关键点：</p><h3 id="ollama的作用" tabindex="-1">Ollama的作用 <a class="header-anchor" href="#ollama的作用" aria-label="Permalink to &quot;Ollama的作用&quot;">​</a></h3><ol><li><p>本地运行LLM：Ollama允许用户在本地计算资源（如GPU）上运行大语言模型。这对于希望完全控制其数据隐私的用户来说非常有用。</p></li><li><p>兼容性：Ollama通常与OpenAI的API兼容，这意味着如果你已经熟悉如何使用OpenAI的API，那么使用Ollama也会非常直观。</p></li><li><p>自定义和优化：用户可以根据自身需求对模型进行微调、优化，甚至部署自己训练的模型。</p></li></ol><h3 id="ollama相关命令和api用法" tabindex="-1">Ollama相关命令和API用法 <a class="header-anchor" href="#ollama相关命令和api用法" aria-label="Permalink to &quot;Ollama相关命令和API用法&quot;">​</a></h3><p>Ollama 提供了一系列命令行工具和API，用于管理、部署和调用本地运行的模型。</p><h4 id="常见命令行用法" tabindex="-1">常见命令行用法 <a class="header-anchor" href="#常见命令行用法" aria-label="Permalink to &quot;常见命令行用法&quot;">​</a></h4><ol><li><p>安装Ollama： 请看官网</p><p><a href="https://ollama.com/download/linux" target="_blank" rel="noreferrer">https://ollama.com/download/linux</a> 通过这个命令，你可以下载并安装Ollama。</p></li><li><p>列出可用模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ollama list</span></span></code></pre></div><p>这个命令会列出你本地已下载的所有模型。</p></li><li><p>启动模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ollama run [model_name]</span></span></code></pre></div><p>启动指定的模型，<code>[model_name]</code>是模型的名称。</p></li></ol><h3 id="端口和api" tabindex="-1">端口和api <a class="header-anchor" href="#端口和api" aria-label="Permalink to &quot;端口和api&quot;">​</a></h3><p>默认端口是11434，api是<a href="http://localhost:11434" target="_blank" rel="noreferrer">http://localhost:11434</a></p><p>兼容 OpenAI API 的 API 路径是：<a href="http://localhost:11434/v1/chat/completions" target="_blank" rel="noreferrer">http://localhost:11434/v1/chat/completions</a></p>',11)])])}const _=l(p,[["render",i]]);export{c as __pageData,_ as default};
